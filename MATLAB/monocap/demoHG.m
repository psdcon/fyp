% This script demonstrates how to use:
% the proposed EM algorithm + pose dictionary learned from Human3.6M
% + the most recent CNN based 2D detector "Hourglass network"
% (https://github.com/anewell/pose-hg-demo)
% to reconstruct 3D human poses from a sequence of images
% The images should have been centered at the subject and cropped
% and all cropped images are saved in a folder
% the heatmaps can be generated by running the HG model:
% (1) go to pose-hg-demo folder
% (2) run in command line: th run-hg.lua ImageFolderName ImageFileExtension
% for example here: th run-hg.lua ../data/tennis jpg
% th run-hg.lua ../data/routine1 png
% make sure Torch has been installed and run correctly

clear
startup

%%

datapath = 'C:\Users\psdco\Videos\Trainings\720x480\0 day2 rout3';
% datapath = 'C:\Users\psdco\Videos\Inhouse\720x480\VID_20161106_112900';

% read images
filelist = dir([datapath '/*.png']);
imagelist = {filelist.name};
nImg = length(imagelist);
image = [];
for i = 1:nImg
    image = cat(4,image,imread(sprintf('%s/%s',datapath,imagelist{i})));
end
disp('Got All Images')

% read heatmaps generated by the Hourglass model
% and stack the heatmaps of all frames
heatmap = [];
for i = 1:nImg
    hm = hdf5read(sprintf('%s/%s.h5',datapath,imagelist{i}(1:end-4)),'heatmap');
    % transpose heatmaps to make it consistent with the MATLAB x-y directions
    heatmap = cat(4,heatmap,permute(hm,[2,1,3]));
end
disp('Got All Heatmaps')

%% load dictionary learned from Human3.6M or CMU

dict = load('dict/poseDict-all-K128'); % a general pose dict
dictDataset = 'hm36m';
% dict = load('dict/tennis_forehand.mat'); % a motion-specific dict
% dictDataset = 'cmu';

% convert dictionary format
% because the joint order in MPII is different from that in Human3.6M
dict = getMPIIdict(dict,dictDataset);
numKp = length(dict.skel.tree);

%% EM
disp('Estiamting Pose From Video')
output = PoseFromVideo('heatmap',heatmap,'dict',dict,'InitialMethod','convex+robust+refine',...
    'alpha',0.5,'beta',20,'gamma',2,'sigma',0.5,'MaxIterAltern',10,'MaxIterEM',20,'verb',false);
% get estimated poses coordinates for EM output
% the estimated 2D joint location is w.r.t. the bounding box
% need to be converted to the original image coordinates
disp('Transforming output')
preds_2d = zeros(2,16,nImg);
preds_3d = zeros(3,16,nImg);
for i = 1:nImg
    center = [size(image,2);size(image,1)]/2;
    scale = size(image,1)/200;
    preds_2d(:,:,i) = transformMPII(output.W_final(2*i-1:2*i,:),center,scale,[size(heatmap,1) size(heatmap,2)],1);
    preds_3d(:,:,i) = output.S_final(3*i-2:3*i,:);
end

%%
save(strcat(datapath, '\preds_2d.mat'), 'preds_2d')

%% show parts real quick
% imshow(image(:,:,:,4));
% hold on
% vis2Dskel(preds_2d(:,:,4),dict.skel);

%% visualize

% nPlot = 4;
% figure('position',[300 300 200*nPlot 200]);

% for i = 1:nImg
% 
%     clf
% 
%     subplot(1,nPlot,1);
%     imshow(image(:,:,:,i));
% % 
%     subplot(1,nPlot,2);
%     imagesc(mat2gray(sum(heatmap(:,:,:,i),3)));
%     axis equal off
% 
%     subplot(1,nPlot,3);
%     imshow(image(:,:,:,i));
% %     hold on;
%     vis2Dskel(preds_2d(:,:,i),dict.skel);
% 
%     subplot(1,nPlot,4);
%     vis3Dskel(preds_3d(:,:,i),dict.skel,'viewpoint',[-45 0]);
% 
%     pause(0.01)
% 
% end

